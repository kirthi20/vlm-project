{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe31fbdf",
   "metadata": {},
   "source": [
    "Adapted from chair.py for thesis experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0311332b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load annotations\n",
    "with open('../data/coco/annotations/instances_val2014.json', 'r') as f:\n",
    "    coco_segments = json.load(f)\n",
    "\n",
    "print(\"Dataset Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30cd50ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load annotations\n",
    "with open('../data/coco/annotations/captions_val2014.json', 'r') as f:\n",
    "    coco_caps = json.load(f)\n",
    "\n",
    "print(\"Dataset Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d979b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# load synonyms.txt\n",
    "with open('synonyms.txt', 'r') as f:\n",
    "    synonyms_txt = f.readlines()\n",
    "\n",
    "print(\"Synonyms Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba4e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "imid_to_objects = {}\n",
    "\n",
    "synonyms = [s.strip().split(', ') for s in synonyms_txt if s.strip()]\n",
    "mscoco_objects = [] #mscoco objects and *all* synonyms\n",
    "inverse_synonym_dict = {}\n",
    "for synonym in synonyms:\n",
    "    mscoco_objects.extend(synonym)\n",
    "    for s in synonym:\n",
    "        inverse_synonym_dict[s] = synonym[0]\n",
    "\n",
    "#Some hard coded rules for implementing CHAIR metrics on MSCOCO\n",
    "        \n",
    "#common 'double words' in MSCOCO that should be treated as a single word\n",
    "coco_double_words = ['motor bike', 'motor cycle', 'air plane', 'traffic light', 'street light', 'traffic signal', 'stop light', 'fire hydrant', 'stop sign', 'parking meter', 'suit case', 'sports ball', 'baseball bat', 'baseball glove', 'tennis racket', 'wine glass', 'hot dog', 'cell phone', 'mobile phone', 'teddy bear', 'hair drier', 'potted plant', 'bow tie', 'laptop computer', 'stove top oven', 'hot dog', 'teddy bear', 'home plate', 'train track']\n",
    "        \n",
    "#Hard code some rules for special cases in MSCOCO\n",
    "#qualifiers like 'baby' or 'adult' animal will lead to a false fire for the MSCOCO object 'person'.  'baby bird' --> 'bird'.\n",
    "animal_words = ['bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'animal', 'cub']\n",
    "#qualifiers like 'passenger' vehicle will lead to a false fire for the MSCOCO object 'person'.  'passenger jet' --> 'jet'.\n",
    "vehicle_words = ['jet', 'train']\n",
    "        \n",
    "#double_word_dict will map double words to the word they should be treated as in our analysis\n",
    "        \n",
    "double_word_dict = {}\n",
    "for double_word in coco_double_words:\n",
    "    double_word_dict[double_word] = double_word\n",
    "for animal_word in animal_words:\n",
    "    double_word_dict['baby %s' %animal_word] = animal_word\n",
    "    double_word_dict['adult %s' %animal_word] = animal_word\n",
    "for vehicle_word in vehicle_words:\n",
    "    double_word_dict['passenger %s' %vehicle_word] = vehicle_word\n",
    "double_word_dict['bow tie'] = 'tie'\n",
    "double_word_dict['toilet seat'] = 'toilet'\n",
    "double_word_dict['wine glas'] = 'wine glass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "564cb8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting annotations from segmentation masks...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "segment_annotations = coco_segments['annotations']\n",
    "\n",
    "#make dict linking object name to ids\n",
    "id_to_name = {} #dict with id to synsets \n",
    "for cat in coco_segments['categories']:\n",
    "    id_to_name[cat['id']] = cat['name']\n",
    "\n",
    "print(\"Getting annotations from segmentation masks...\")\n",
    "for i, annotation in enumerate(segment_annotations):\n",
    "    imid = annotation['image_id']\n",
    "            \n",
    "    node_word = inverse_synonym_dict[id_to_name[annotation['category_id']]]\n",
    "    if imid not in imid_to_objects:\n",
    "        imid_to_objects[imid] = []\n",
    "    imid_to_objects[imid].append(node_word)\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd1d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet', quiet=True)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def caption_to_words(caption):\n",
    "    '''\n",
    "    Input: caption\n",
    "    Output: MSCOCO words in the caption\n",
    "    '''\n",
    "\n",
    "    #standard preprocessing\n",
    "    words = nltk.word_tokenize(caption.lower())\n",
    "    words = [lemmatizer.lemmatize(w, pos='n') for w in words]\n",
    "\n",
    "    #replace double words\n",
    "    i = 0\n",
    "    double_words = []\n",
    "    idxs = []\n",
    "    while i < len(words):\n",
    "        idxs.append(i) \n",
    "        double_word = ' '.join(words[i:i+2])\n",
    "        if double_word in double_word_dict: \n",
    "            double_words.append(double_word_dict[double_word])\n",
    "            i += 2\n",
    "        else:\n",
    "            double_words.append(words[i])\n",
    "            i += 1\n",
    "    words = double_words\n",
    "\n",
    "    #toilet seat is not chair (sentences like \"the seat of the toilet\" will fire for \"chair\" if we do not include this line)\n",
    "    if ('toilet' in words) & ('seat' in words): words = [word for word in words if word != 'seat']\n",
    "\n",
    "    #get synonyms for all words in the caption\n",
    "    idxs = [idxs[idx] for idx, word in enumerate(words) \\\n",
    "            if word in set(mscoco_objects)]\n",
    "    words = [word for word in words if word in set(mscoco_objects)]\n",
    "    node_words = []\n",
    "    for word in words:\n",
    "        node_words.append(inverse_synonym_dict[word])\n",
    "    #return all the MSCOCO objects in the caption\n",
    "    return words, node_words, idxs, double_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd415c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 203564, 'id': 37, 'caption': 'A bicycle replica with a clock as the front wheel.'}\n"
     ]
    }
   ],
   "source": [
    "caption_annotations = coco_caps['annotations']\n",
    "\n",
    "print (caption_annotations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88580839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yippee!!\n",
      "yippee!!\n",
      "yippee!!\n",
      "yippee!!\n",
      "yippee!!\n"
     ]
    }
   ],
   "source": [
    "for i, annotation in enumerate(caption_annotations):\n",
    "    imid = annotation['image_id']\n",
    "            \n",
    "    _, node_words, _, _ = caption_to_words(annotation['caption'])\n",
    "    # note here is update, so call get_annotations_from_segments first\n",
    "    if imid not in imid_to_objects:\n",
    "        imid_to_objects[imid] = []\n",
    "    imid_to_objects[imid].extend(node_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate\n",
    "for imid in imid_to_objects:\n",
    "    imid_to_objects[imid] = list(set(imid_to_objects[imid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10511e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(184613 in imid_to_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb18b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save imid_to_objects\n",
    "with open('imid_to_objects.json', 'w') as f:\n",
    "    json.dump(imid_to_objects, f)\n",
    "print(\"Saved imid_to_objects.json successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
